#version: '3.9'

services:
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    image: zoometf-backend
    env_file:
      - ./backend/.env
    depends_on:
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      db:
        condition: service_healthy
    ports:
      - "8000:8000"
      - "1025:1025"  # Conserv√© pour compatibilit√©
    environment:
      - PYTHONPATH=/zoom-etf/backend
      - PYTHONUNBUFFERED=1  # Ajout important pour les logs Python
    volumes:
      - ./backend:/zoom-etf/backend
    restart: unless-stopped
    command: uvicorn backend.app.main:app --host 0.0.0.0 --port 8000 --reload
    healthcheck:  # Nouveau healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - zoometf-net

  # OLLAMA SIMPLIFI√â - PAS DE COMMANDES COMPLEXES
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - zoometf-net
    environment:
      - OLLAMA_HOST=0.0.0.0
    # HEALTHCHECK SIMPLE - v√©rifie seulement que le processus r√©pond
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s

  # OLLAMA_INIT CORRIG√â - syntaxe shell valide
  ollama_init:
    image: curlimages/curl:latest
    depends_on:
      ollama:
        condition: service_healthy
    command: >
      sh -c "
      # Phase 1: Attente de l'API Ollama
      echo 'üîç Phase 1: Attente de l API Ollama...' &&
      for i in 1 2 3 4 5 6 7 8 9 10; do
        if curl -s http://ollama:11434/api/version > /dev/null; then
          echo '‚úÖ API Ollama disponible' &&
          break
        fi &&
        echo '‚è≥ API Ollama pas encore pr√™te, attente 15s... ($$i/10)' &&
        sleep 15
      done &&
      
      # Phase 2: V√©rification mod√®le existant
      echo 'üîç Phase 2: V√©rification mod√®le existant...' &&
      if curl -s http://ollama:11434/api/tags | grep -q 'llama3:8b'; then
        echo '‚úÖ Mod√®le llama3:8b d√©j√† pr√©sent' &&
        exit 0
      fi &&
      
      # Phase 3: T√©l√©chargement
      echo 'üì• Phase 3: T√©l√©chargement du mod√®le llama3:8b...' &&
      echo '‚ö†Ô∏è  Dur√©e estim√©e: 10-30 minutes' &&
      curl -X POST http://ollama:11434/api/pull \
        -H 'Content-Type: application/json' \
        -d '{\"name\": \"llama3:8b\"}' &&
      
      # Phase 4: V√©rification finale
      echo 'üîç Phase 4: V√©rification finale...' &&
      if curl -s http://ollama:11434/api/tags | grep -q 'llama3:8b'; then
        echo 'üéâ SUCC√àS: Mod√®le llama3:8b install√©' &&
        echo '‚úÖ Ollama op√©rationnel'
      else
        echo '‚ùå √âCHEC: Mod√®le non d√©tect√© apr√®s installation' &&
        exit 1
      fi
      "
    networks:
      - zoometf-net
    restart: "no"

  redis:
    image: redis:alpine
    restart: unless-stopped
    healthcheck:  # Healthcheck ajout√©
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 3
    volumes:  # Persistence des donn√©es
      - redis_data:/data
    networks:
      - zoometf-net

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.6.2
    ports:
      - "9200:9200"
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1g -Xmx1g  # Allocation m√©moire
      - xpack.security.enabled=false  # S√©curit√© d√©sactiv√©e pour le dev
    restart: unless-stopped
    ulimits:  # Configuration importante pour ES
      memlock:
        soft: -1
        hard: -1
    volumes:  # Persistence des donn√©es
      - es_data:/usr/share/elasticsearch/data
    healthcheck:  # Healthcheck ajout√©
      test: ["CMD", "curl", "-f", "http://localhost:9200/_cluster/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - zoometf-net

  prometheus:
    image: prom/prometheus
    # volumes:
    #  - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    restart: unless-stopped
    networks:
      - zoometf-net

  db:
    image: postgres:14
    container_name: zoometf-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: zoomuser
      POSTGRES_PASSWORD: zoompass
      POSTGRES_DB: zoometf
      PGDATA: /var/lib/postgresql/data/pgdata  # Meilleure organisation
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
      - ./backend/app/db/init:/docker-entrypoint-initdb.d  # Montage du dossier d'initialisation
    healthcheck:  # Healthcheck ajout√©
      test: ["CMD-SHELL", "pg_isready -U zoomuser -d zoometf"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - zoometf-net

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    restart: unless-stopped
    volumes:  # Persistence des donn√©es
      - grafana_data:/var/lib/grafana
    networks:
      - zoometf-net

  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    working_dir: /app
    image: zoometf-frontend
    env_file:
      - ./frontend/.env
    #volumes:
      #- ./frontend/src:/app/src           # Uniquement les sources
      #- ./frontend/public:/app/public      # Uniquement les assets
      #- ./frontend/package.json:/app/package.json  # package.json explicitement
      #- /app/node_modules
    ports:
      - "80:3000" # Port expos√© pour le frontend
    command: sh -c "npm install && npm start"
    environment:
      - CHOKIDAR_USEPOLLING=true
      - NODE_ENV=development
    restart: unless-stopped
    healthcheck:
      disable: true  # << D√©sactivation du healthcheck pour √©viter l'erreur
    networks:
      - zoometf-net  # << Correction ici : il manquait le nom du r√©seau


  docs:
    image: nginx:alpine
    volumes:
      - ./docs/site:/usr/share/nginx/html:ro
    ports:
      - "4000:80"
    restart: unless-stopped
    networks:
      - zoometf-net

  pgadmin:
    image: dpage/pgadmin4
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"
    volumes:  # Persistence des donn√©es
      - pgadmin_data:/var/lib/pgadmin
    depends_on:  # D√©pendance explicite
      db:
        condition: service_healthy
    networks:
      - zoometf-net

volumes:
  zoometf_db_data:
  redis_data:
  es_data:
  grafana_data:
  pgadmin_data:
  pg_data:

networks:
  zoometf-net:
    driver: bridge
